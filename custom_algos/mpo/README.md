### MPO - Maximum A Posteriori Policy Optimisation

Implementation taken from [https://github.com/daisatojp/mpo](https://github.com/daisatojp/mpo)
